{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practico 2 - Exploración Y Curación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este práctico vamos a continuar el análisis del práctico anterior, aplicandole algunas técnicas de curación y manipulación de datos para profundizar y extender el analisis.\n",
    "\n",
    "Vamos a enfocarnos en las columnas que tienen mas potencial de analisis y ver de que otras maneras podemos explorar el dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consignas\n",
    "### Para columnas Numéricas\n",
    "\n",
    "Aplicar los análisis propuestos en las consignas 1 y 2 sobre: *budget, revenue, popularity, vote_average, vote_count, runtime*\n",
    "\n",
    "1. Establecer reglas de filtrado para los datos anomalos que encuentren en cada variable. \n",
    "    - Cada regla tiene que estar justificada por escrito, dando alguna explicación correspondiente a la naturaleza de la variable o cómo se presenta en el dataset.\n",
    "\n",
    "    - Según el estudio de outliers que hicieron en el práctico anterior, definan filtros para eliminar los outliers siempre que sea funcional al análisis. Justifiquen el porqué de los límites que establezcan. Aquí quiero que entiendan por qué el dato que ven es un outlier y lo expliquen (En caso de que lo hayan hecho en el práctico anterior, no es necesario repetir el análisis).\n",
    "\n",
    "    - Noten que quiero diferenciar datos erroneos de outliers porque no son lo mismo. Las reglas de filtrado no deberian tratarlos como si lo fueran, o al menos se deberia aclarar si se hacen ambas al mismo tiempo. Traten a las reglas de filtrado como documentacion del proceso\n",
    "\n",
    "2. Escalado\n",
    "    - Las distintas columnas numéricas del dataset son distintas en escala y eso dificulta analizar como se relacionan. Elijan algún método para escalarlas entre los que hayan estudiado (StandardScaler, MinMaxScaler, etc.) y aplíquenlo. \n",
    "\n",
    "3. Analisis\n",
    "    - Luego de aplicar las reglas, hagan visualizaciones de las correlaciones entre las distintas columnas y expliquen lo que se vea. El sentido de escalar las variables es poder relacionarlas entre sí, y encontrar la verdadera relación entre ellas, sin la influencia de los rangos que puede tomar cada una.\n",
    "\n",
    "    - Hacer analisis de correlacion y responder: \n",
    "        - ¿Cómo cambia la relación entre popularity y vote_average con las variables escaladas a un mismo rango?\n",
    "        - Y entre budget y revenue?\n",
    "        - Entre runtime y budget?\n",
    "        - (Agregar mas comparaciones si lo desean)\n",
    "        \n",
    "4. Construir una variable nueva\n",
    "    - A diferencia de popularity, vote_average tiene una distribución cercana a la normal. Esto la hace un poco más atractiva a la hora de analizar tendencias. Vamos a utilizar la variable vote_count para darle más peso a vote_average. Sabemos que no es lo mismo una película que fue votada 4 veces que una que fue votada 400 veces. El voto tiene más peso en el último caso. Quiero que construyan una nueva variable \"vote_combination\" que esté conformada por la combinación entre vote_average y vote_count. Para combinarlas, van a hacer lo siguiente:\n",
    "        1. Escalar vote_count entre 0 y 1\n",
    "        2. Escalar vote_average entre 0 y 1 usando una instancia de scaler nueva.\n",
    "        3. calcular el producto entre ambas columnas escaladas\n",
    "        4. des-escalar la columna resultante con el scaler de vote_average\n",
    "        5. Analizar resultados.\n",
    "\n",
    "    - Responder:\n",
    "        - ¿Hay diferencias entre la nueva columna y vote_average?\n",
    "        - ¿Cómo se diferencia con popularity?\n",
    "\n",
    "\n",
    "### Para columnas Categóricas\n",
    "\n",
    "Elijan al menos una de las siguientes columnas para aplicarle el análisis propuesto en la consigna 1: *genres, original_language, production_companies, production_countries, spoken_languages*\n",
    "\n",
    "1. Aplicar one-hot encoding o dummy\n",
    "\n",
    "    - La mayoría de las variables tienen muchas categorías, por lo que el resultado del encoding va a ser una matriz muy extensa en cantidad de columnas. Para evitar esto, vamos a elegir aquellas categorías que sean predominantes y aplicar sobre ellas el análisis. Sobre género, por ejemplo, pueden elegir los 10 o 20 géneros con más películas presentes.\n",
    "\n",
    "    - Para poder realizar este ejercicio es necesario tener un set único de categorías para las columnas que contienen listas. (realizado en el práctico anterior)\n",
    "\n",
    "2. Agregar las palabras clave de cada película\n",
    "    - Usar el archivo 'keywords.csv' para agregar a cada película una columna nueva que contenga una lista con las palabras clave. Por ejemplo, para el caso de Toy Story, la columna \"keywords\" debería ser: ['jealousy', 'toy', 'boy', 'friendship', 'friends', 'rivalry', 'boy next door', 'new toy', 'toy comes to life'].\n",
    "\n",
    "    - Una vez hecha la columna, usar los métodos aplicados sobre las otras columnas con listas para analizar la columna keywords.\n",
    "\n",
    "    - Responder: ¿Hay alguna sensación o grupo de palabras con cierta intención que sea predominante?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
